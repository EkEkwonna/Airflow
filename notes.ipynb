{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we set up aiflow virtual environment and entered the following according to your python version \n",
    "\n",
    "<code>pip install 'apache-airflow==2.10.4' \\\n",
    " --constraint \"https://raw.githubusercontent.com/apache/airflow/constraints-2.10.4/constraints-3.9.txt\"</code>\n",
    "\n",
    "set up home environment variable to current directory with \n",
    "\n",
    "<code> export AIRFLOW_HOME=~/airflow</code>\n",
    "\n",
    "And we can initialise an SQLite database with a log folder and configuration files \n",
    "\n",
    "<code> airflow db init </code>\n",
    "\n",
    "Airflow webserver \n",
    "\n",
    "<code>airflow webserver -p8080</code>\n",
    "\n",
    "will open a link that you can select to access the webserver <code>http://0.0.0.0:8080/</code>\n",
    "\n",
    "we can create the login details/account use the following template: \n",
    "\n",
    "<code>airflow users create --username admin --firstname firstname --lastname lastname --role Admin --email admin@domain.com</code>\n",
    "\n",
    "(Pass:1234)\n",
    "\n",
    "for additional support use:  <code>airflow users create --help</code>\n",
    "\n",
    "to delete a user account <code>airflow users delete -u username </code>\n",
    "\n",
    "Scheduler needs to be running to allow tasks to execute\n",
    "\n",
    "in a new terminal run <code>airflow scheduler</code> to reset the scheduler \n",
    "\n",
    "To turn on DAG toggle the button, from the tree view we can see the dag has multiple tasks\n",
    "\n",
    "![ExampleDag](Images/ExampleDag.png)\n",
    "\n",
    "The toggle allows us to pause and unpause the Dag\n",
    "\n",
    "DAG names have to be unique\n",
    "\n",
    "\n",
    "![DagHistory](Images/DagHistory.png)\n",
    "\n",
    "![DagHistory2](Images/DagHistory2.png)\n",
    "\n",
    "\n",
    "![DAGStates](Images/DagStates.png)\n",
    "\n",
    "Gant view can be used to detect whether a task has ran longer than expected \n",
    "\n",
    "![GantView](Images/GanttView.png)\n",
    "\n",
    "When a task has failed you can click on the page and clear task \n",
    "\n",
    "![ClearTask](Images/ClearTask.png)\n",
    "\n",
    "Note what you clear is the task that will be restarted: \n",
    "* Downstream (DAG tasks following the failed task including the task that failed)\n",
    "* Upstream (tasks prior to the task that failed within the DAG)\n",
    "* Past ( Looks at task triggered in the past)\n",
    "* Future ( Looks at tasks scheduled in the future)\n",
    "* Recursive (Included subDAGs)\n",
    "\n",
    "We can insert Variables (key value pairs containing string that we can use)\n",
    "\n",
    "![Variables](Images/AirflowVariables.png)\n",
    "\n",
    "\n",
    "We can also use connections to establish configuratigons with external resources \n",
    "\n",
    "![Connections](Images/AirflowConnections.png)\n",
    "\n",
    "May need to test this out to confirm we can connect via EXASOL\n",
    "\n",
    "![MySQLForm](Images/MySQLForm.png)\n",
    "\n",
    "When you clear a state of a task or DAG you restart it\n",
    "\n",
    "DAG Dependencies \n",
    "\n",
    "![DagDepencencies](Images/DagDependencies.png)\n",
    "\n",
    "* Dataset Dependency - these DAGs are triggered once there's a detection of a dataset being updated (can be 1 or more)\n",
    "* \n",
    "\n",
    "\n",
    "\n",
    "DATASET DEPENDENCY EXAMPLE \n",
    "\n",
    "\n",
    "In this example we will have created a DAG that feeds 2 separate tables in S3\n",
    "DAG 2 will be triggered once there is a detection of the dataset being updated \n",
    "\n",
    "DAG 1\n",
    "<pre><code id=\"python_code\">\n",
    "\n",
    "from airflow.decorators import dag, task \n",
    "from airflow import Dataset \n",
    "from datetime import datetime \n",
    "\n",
    "data_a = Dataset(\"s3://bucket_a/data_a)\n",
    "data_b = Dataset(\"s3://bucket_b/data_b)\n",
    "\n",
    "#define the DAG object\n",
    "@dag(start_date = datetime(2025,1,1),schedule = @daily,catchup = False)\n",
    "def producer():\n",
    "\n",
    "    #outlets decide where the destination \n",
    "    @task(outlets = [data_a])\n",
    "    def update_a():\n",
    "        print(\"Data A has been updated\")\n",
    "\n",
    "    @task(outlets=[data_b])\n",
    "    def update_b():\n",
    "        print(\"Data B has been updated\")\n",
    "\n",
    "    #Defines the dependency \n",
    "    update_a() >> update_b()\n",
    "\n",
    "producer()\n",
    "</code></pre>\n",
    "\n",
    "In the same folder you can create a second file which represents our downstream DAG\n",
    "\n",
    "DAG 2\n",
    "Note we can also utilise connections established in the admin settings to update a dataset \n",
    "\n",
    "<pre><code id = \"python_code\">\n",
    "\n",
    "from airflwo.decorateors import dag,task\n",
    "from airflow import Dataset \n",
    "from datetime imoprt datetime \n",
    "\n",
    "data_a = Dataset(\"s3://bucket_a/data_a)\n",
    "data_b = Dataset(\"s3://bucket_b/data_b)\n",
    "\n",
    "@dag(start_date = datetime(2025,1,1),schedule = [data_a,data_b], catchup = False)\n",
    "def consumer():\n",
    "\n",
    "    @task\n",
    "    def run():\n",
    "        print(\"run\")\n",
    "        \n",
    "    run()\n",
    "\n",
    "consumer()\n",
    "\n",
    "</code></pre>\n",
    "\n",
    "When setting up dependencies you can set up the DAGS so that the Upstream DAG triggers the Downstream DAG once it's over\n",
    "\n",
    "![Upstreamanddownstrea](Images/UpstreamandDownstreamDAG.png)\n",
    "\n",
    "\n",
    "------------------------------\n",
    "\n",
    "Airflow in Docker\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
