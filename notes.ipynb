{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we set up aiflow virtual environment and entered the following according to your python version \n",
    "\n",
    "<code>pip install 'apache-airflow==2.10.4' \\\n",
    " --constraint \"https://raw.githubusercontent.com/apache/airflow/constraints-2.10.4/constraints-3.9.txt\"</code>\n",
    "\n",
    "set up home environment variable to current directory with \n",
    "\n",
    "<code> export AIRFLOW_HOME=~/airflow</code>\n",
    "\n",
    "And we can initialise an SQLite database with a log folder and configuration files \n",
    "\n",
    "<code> airflow db init </code>\n",
    "\n",
    "Airflow webserver \n",
    "\n",
    "<code>airflow webserver -p8080</code>\n",
    "\n",
    "will open a link that you can select to access the webserver <code>http://0.0.0.0:8080/</code>\n",
    "\n",
    "we can create the login details/account use the following template: \n",
    "\n",
    "<code>airflow users create --username admin --firstname firstname --lastname lastname --role Admin --email admin@domain.com</code>\n",
    "\n",
    "(Pass:1234)\n",
    "\n",
    "for additional support use:  <code>airflow users create --help</code>\n",
    "\n",
    "to delete a user account <code>airflow users delete -u username </code>\n",
    "\n",
    "Scheduler needs to be running to allow tasks to execute\n",
    "\n",
    "in a new terminal run <code>airflow scheduler</code> to reset the scheduler \n",
    "\n",
    "To turn on DAG toggle the button, from the tree view we can see the dag has multiple tasks\n",
    "\n",
    "![ExampleDag](Images/ExampleDag.png)\n",
    "\n",
    "The toggle allows us to pause and unpause the Dag\n",
    "\n",
    "DAG names have to be unique\n",
    "\n",
    "\n",
    "![DagHistory](Images/DagHistory.png)\n",
    "\n",
    "![DagHistory2](Images/DagHistory2.png)\n",
    "\n",
    "\n",
    "![DAGStates](Images/DagStates.png)\n",
    "\n",
    "Gant view can be used to detect whether a task has ran longer than expected \n",
    "\n",
    "![GantView](Images/GanttView.png)\n",
    "\n",
    "When a task has failed you can click on the page and clear task \n",
    "\n",
    "![ClearTask](Images/ClearTask.png)\n",
    "\n",
    "Note what you clear is the task that will be restarted: \n",
    "* Downstream (DAG tasks following the failed task including the task that failed)\n",
    "* Upstream (tasks prior to the task that failed within the DAG)\n",
    "* Past ( Looks at task triggered in the past)\n",
    "* Future ( Looks at tasks scheduled in the future)\n",
    "* Recursive (Included subDAGs)\n",
    "\n",
    "We can insert Variables (key value pairs containing string that we can use)\n",
    "\n",
    "![Variables](Images/AirflowVariables.png)\n",
    "\n",
    "\n",
    "We can also use connections to establish configuratigons with external resources \n",
    "\n",
    "![Connections](Images/AirflowConnections.png)\n",
    "\n",
    "May need to test this out to confirm we can connect via EXASOL\n",
    "\n",
    "![MySQLForm](Images/MySQLForm.png)\n",
    "\n",
    "When you clear a state of a task or DAG you restart it\n",
    "\n",
    "DAG Dependencies \n",
    "\n",
    "![DagDepencencies](Images/DagDependencies.png)\n",
    "\n",
    "* Dataset Dependency - these DAGs are triggered once there's a detection of a dataset being updated (can be 1 or more)\n",
    "* Trigger Dependency -  \n",
    "* Sensor Dependency - \n",
    "\n",
    "\n",
    "When setting up dependencies you can set up the DAGS so that the Upstream DAG triggers the Downstream DAG once it's over\n",
    "\n",
    "![Upstreamanddownstrea](Images/UpstreamandDownstreamDAG.png)\n",
    "\n",
    "\n",
    "DATASET DEPENDENCY EXAMPLE \n",
    "\n",
    "\n",
    "In this example we will have created a DAG that feeds 2 separate tables in S3\n",
    "DAG 2 will be triggered once there is a detection of the dataset being updated \n",
    "\n",
    "DAG 1\n",
    "<pre><code id=\"python_code\">\n",
    "\n",
    "from airflow.decorators import dag, task \n",
    "from airflow import Dataset \n",
    "from datetime import datetime \n",
    "\n",
    "data_a = Dataset(\"s3://bucket_a/data_a)\n",
    "data_b = Dataset(\"s3://bucket_b/data_b)\n",
    "\n",
    "#define the DAG object\n",
    "@dag(start_date = datetime(2025,1,1),schedule = @daily,catchup = False)\n",
    "def producer():\n",
    "\n",
    "    #outlets decide where the destination \n",
    "    @task(outlets = [data_a])\n",
    "    def update_a():\n",
    "        print(\"Data A has been updated\")\n",
    "\n",
    "    @task(outlets=[data_b])\n",
    "    def update_b():\n",
    "        print(\"Data B has been updated\")\n",
    "\n",
    "    #Defines the dependency \n",
    "    update_a() >> update_b()\n",
    "\n",
    "producer()\n",
    "</code></pre>\n",
    "\n",
    "In the same folder you can create a second file which represents our downstream DAG\n",
    "\n",
    "DAG 2\n",
    "Note we can also utilise connections established in the admin settings to update a dataset \n",
    "\n",
    "<pre><code id = \"python_code\">\n",
    "\n",
    "from airflwo.decorateors import dag,task\n",
    "from airflow import Dataset \n",
    "from datetime imoprt datetime \n",
    "\n",
    "data_a = Dataset(\"s3://bucket_a/data_a)\n",
    "data_b = Dataset(\"s3://bucket_b/data_b)\n",
    "\n",
    "@dag(start_date = datetime(2025,1,1),schedule = [data_a,data_b], catchup = False)\n",
    "def consumer():\n",
    "\n",
    "    @task\n",
    "    def run():\n",
    "        print(\"run\")\n",
    "        \n",
    "    run()\n",
    "\n",
    "consumer()\n",
    "\n",
    "</code></pre>\n",
    "\n",
    "\n",
    "\n",
    "TRIGGER OPERATOR  EXAMPLE \n",
    "\n",
    "We will create 2 DAGs 1 controller.py that will trigger the target.py DAG\n",
    "\n",
    "<pre><code>\n",
    "\n",
    "from airflwo.decorateors import dag,task\n",
    "from airflow.operators.trigger_dagrun import TriggerDagRunOperator\n",
    "from datetime imoprt datetime \n",
    "\n",
    "@dag(start_date=datetime(2025,1,1),schedule = '@None',catchup= False)\n",
    "def controller():\n",
    "\n",
    "    @task\n",
    "    def start():\n",
    "        print(\"start\")\n",
    "\n",
    "    trigger = TriggerDagRunOperator(\n",
    "        task_id='trigger_target_dag',         \n",
    "        trigger_dag_id = 'target',              # The DAG ID of the DAG you want to trigger\n",
    "        conf = {\"message\":\"my_data\"},           # configuration parameter with data you want to pass to trigger DAG\n",
    "        wait_for_completion = True              # will wait till the end of completion of this task before the trigger is set \n",
    "    )\n",
    "\n",
    "    @task\n",
    "    def done():\n",
    "        print(\"done)\n",
    "\n",
    "</code></pre>\n",
    "\n",
    "Trigger.py will take a shape similar to the following : \n",
    "\n",
    "<pre><code>\n",
    "\n",
    "from airflow.decoraters import dag,task\n",
    "from datetime import datetime \n",
    "\n",
    "@dag(start_date=datetime(2025,1,1),schedule='@None' ,catchup = False)\n",
    "def target():\n",
    "\n",
    "    @task\n",
    "    def message(dag_run = None):\n",
    "        print(dag_run.conf.get(\"message\"))\n",
    "\n",
    "    message()\n",
    "\n",
    "target()\n",
    "\n",
    "</code></pre>\n",
    "\n",
    "With a schedule set to None this DAG won't run periodically and is only triggered by the first controller DAG\n",
    "\n",
    "\n",
    "EXTERNAL TASK SENSOR EXAMPLE\n",
    "\n",
    "You can use External task sensors to ensure a multitude of tasks are completed before triggering specific tasks in a DAG \n",
    "in this example waiting_for_a_b_c.py is a DAG that will track the completion of dag_a.py , dag_b.py and dag_c.py \n",
    "\n",
    "Picture 3 dags defined as the following : \n",
    "\n",
    "dag_a\n",
    "<code><pre>\n",
    "\n",
    "from airflow.decorators import dag, task\n",
    "from datetime import datetime\n",
    "\n",
    "@dag(start_date = datetime(2025,1,1),schedule = '@daily',catchup= False)\n",
    "def dag_a():\n",
    "\n",
    "    @task\n",
    "    def task_a():\n",
    "        print(\"task_a\")\n",
    "\n",
    "    task_a()\n",
    "\n",
    "dag_a()\n",
    "\n",
    "</code></pre>\n",
    "\n",
    "\n",
    "dag_b\n",
    "<code><pre>\n",
    "\n",
    "from airflow.decorators import dag, task\n",
    "from datetime import datetime\n",
    "\n",
    "@dag(start_date = datetime(2025,1,1),schedule = '@daily',catchup= False)\n",
    "def dag_b():\n",
    "\n",
    "    @task\n",
    "    def task_b():\n",
    "        print(\"task_b\")\n",
    "\n",
    "    task_b()\n",
    "\n",
    "dag_b()\n",
    "\n",
    "</code></pre>\n",
    "\n",
    "dag_c\n",
    "<code><pre>\n",
    "\n",
    "from airflow.decorators import dag, task\n",
    "from datetime import datetime\n",
    "\n",
    "@dag(start_date = datetime(2025,1,1),schedule = '@daily',catchup= False)\n",
    "def dag_c():\n",
    "\n",
    "    @task\n",
    "    def task_c():\n",
    "        print(\"task_c\")\n",
    "\n",
    "    task_c()\n",
    "\n",
    "dag_c()\n",
    "\n",
    "</code></pre>\n",
    "\n",
    "You can create a DAG that waits on the external tasks before proceeding a following task\n",
    "\n",
    "waiting_for_a_b_c.py\n",
    "<pre><code>\n",
    "\n",
    "from airflow.decorateros import dag,task\n",
    "from airflow.sensors.external_task import ExternalTaskSensor\n",
    "from datetime import datetime \n",
    "\n",
    "@dag(start_date=datetime(2025,1,1),schedule='@None',catchup = False)\n",
    "def waiting_for_a_b_c():\n",
    "\n",
    "    waiting_for_a = ExternalTaskSensor(\n",
    "        task_id = 'waiting_for_a',\n",
    "        external_dag_id = 'dag_a',\n",
    "        external_task_id = 'task_a'\n",
    "    )\n",
    "\n",
    "    waiting_for_b = ExternalTaskSensor(\n",
    "        task_id = 'waiting_for_b',\n",
    "        external_dag_id = 'dag_b',\n",
    "        external_task_id = 'task_b'\n",
    "    )\n",
    "\n",
    "    waiting_for_c = ExternalTaskSensor(\n",
    "        task_id = 'waiting_for_c',\n",
    "        external_dag_id = 'dag_c',\n",
    "        external_task_id = 'task_c'\n",
    "    )\n",
    "\n",
    "    @task\n",
    "    def next_a():\n",
    "        print(\"a\")\n",
    "\n",
    "    @task\n",
    "    def next_b():\n",
    "        print(\"b\")\n",
    "\n",
    "    @task\n",
    "    def next_c():\n",
    "        print(\"c\")\n",
    "\n",
    "    waiting_for_a >> next_a >> done()\n",
    "    waiting_for_b >> next_b >> done()\n",
    "    waiting_for_c >> next_c >> done()\n",
    "\n",
    "waiting_for_a_b_c()\n",
    "\n",
    "</code></pre>\n",
    "\n",
    "Note: External task sensors have the following configuraitons : \n",
    "* timeout - can help set a timeframe for the sensor to fail \n",
    "* poke_interval - which allows you to set a duration of time to check the external dag stage \n",
    "* mode = \"reschedule\" - this will mean the sensor DAG will be rescheduled at the next poke interval (if a condiction isn't met) instead of running for a long duration\n",
    "\n",
    "Alternatively you can build DAG to ahve the following dependency \n",
    "\n",
    "<code>wait_task >> [success_task , fail_task] </code>\n",
    "\n",
    "where the wait_task is our sensor success_task has trigger rule all_success and is followed when sensors succeeds and fail_task with all_failed trigger rules handles what happened when sensor returns false or timeouts \n",
    "\n",
    "------------------------------\n",
    "\n",
    "Airflow in Docker\n",
    "\n",
    "(Dependencies: Docker, Docker compose)\n",
    "\n",
    "Docker - open source platform allows developers to build,test,deploy appliations quickly. Using containsers to package software into standardized units that can run.\n",
    "\n",
    "Docker Compose - is a tool that helps you define and share multi-container applications. \n",
    "With Compse, you can create a YAML file to define the service and with a single command, you can spin everything up or tear it all down \n",
    "\n",
    "The big advantage of using Compose is you can define your application stack in a file, keep it at the root of your project repository,\n",
    "and easilyt enable someone else ot contribute to your project. (All someone needs to do is clone the repository and star the app using Compose)\n",
    "\n",
    "\n",
    "Fetch docker Compose yaml file: (Guidelines used for inspiration)\n",
    "\n",
    "<code>curl -LfO 'https://airflow.apache.org/docs/apache-airflow/2.10.4/docker-compose.yaml'</code>\n",
    "\n",
    "From within the YAML file: \n",
    "1. Update the AIRFLOW_CORE_EXECUTOR From CeleryExecutor to LocalExecutor\n",
    "2. Delete the AIRFLOW__CELERY__RESULT_BACKEND and AIRFLOW__CELERY__BROKER_URL variables from the environment\n",
    "3. remove the redis key under the dependency for airflow-common-depends-on\n",
    "4. remove the entire redis definition from the services section\n",
    "5. remove the airflow-worker section referncing the \"celery worker\" command\n",
    "6. remove the flower section from the services \n",
    "\n",
    "\n",
    "Folders will be required for DAGs, logs and Plugins \n",
    "\n",
    "from within the folder of interest run \n",
    "<code>mkdir -p ./dags ./logs ./plugins ./config</code>\n",
    "\n",
    "To download all the necessary docker images and initialise the database :\n",
    "(Ensure Docker is running when you enter this command)\n",
    "(Check the Docker >> Resources >>> max Memory settings to ensure there's 4GB> (7GB~) allocated)\n",
    "\n",
    "<code>docker-compose up airflow-init</code>\n",
    "\n",
    "Database initialisation is complete. \n",
    "\n",
    "Allows us to run containers in background ( in detached mode)\n",
    "\n",
    "<code>docker-compose up -d</code>\n",
    "\n",
    "\n",
    "\n",
    "This shows us we have an Airflow container, An airflow scheduler and a postgres database : \n",
    "\n",
    "![](Images/DockerContainers.png)\n",
    "\n",
    "With these running we can check the Airflow Webserver via localhost:8080 port \n",
    "\n",
    "We can also view which containers are running in the airflow project : \n",
    "\n",
    "![](Images/ContainersLive.png)\n",
    "\n",
    "NOTE: (Check the YAML file to detect the URL required for the Airflow UI and the credentials to the admin account)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "------------------------\n",
    "\n",
    "WAtch core concepts + task lifecycle + basic architecture \n",
    "\n",
    "-----------------\n",
    "\n",
    "Airflow DAG with Python operator \n",
    "\n",
    "\n",
    "--------\n",
    "\n",
    "\n",
    "Data sharing with Xcoms \n",
    "\n",
    "\n",
    "----------\n",
    "\n",
    "\n",
    "Airflow task APIs\n",
    "\n",
    "------\n",
    "\n",
    "Airflow Catchup and Backfill \n",
    "\n",
    "----\n",
    "\n",
    "Can we create a CI/CD pipelines ?\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
